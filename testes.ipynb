{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testes.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JonReyz/aspectClustering/blob/master/testes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cHyymNwJtFB",
        "colab_type": "code",
        "outputId": "cac8f108-227f-4078-c2b3-61edfbd39e8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import nltk\n",
        "import csv\n",
        "nltk.download('punkt')\n",
        "import gensim\n",
        "from nltk.corpus import wordnet \n",
        "from nltk.tokenize import sent_tokenize, word_tokenize \n",
        "from gensim.models import Word2Vec "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ93M03-Ose3",
        "colab_type": "code",
        "outputId": "67963b79-3569-476e-b80b-d37d2e90f7eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_lg==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-2.1.0/en_core_web_lg-2.1.0.tar.gz#egg=en_core_web_lg==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlmnCFeP7BgY",
        "colab_type": "code",
        "outputId": "0fc6c4a4-afea-40d2-b63a-242de0c99e92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "!pip install spacy en_core_web_md\n",
        "!pip install <model_s3_url>"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.1.9)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement en_core_web_md (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for en_core_web_md\u001b[0m\n",
            "/bin/bash: -c: line 0: syntax error near unexpected token `newline'\n",
            "/bin/bash: -c: line 0: `pip install <model_s3_url>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R0eCID5NcIC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#import spacy\n",
        "#!spacy download en_core_web_md\n",
        "\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()\n",
        "tokens = nlp(str(data))\n",
        "\n",
        "lista = []\n",
        "#print(tokens)\n",
        "for token1 in tokens:\n",
        "  lista.append(token1)\n",
        "  #for token2 in tokens:\n",
        "   # print(token1.text, token2.text,token1.similarity(token2))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdU30UWe1mEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lista\n",
        "#for words in tokens:\n",
        "  #print(words, words.lemma_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Og8SwUU83PL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#depois de ter rodado um nlp\n",
        "def str2list(str):\n",
        "  lista = []\n",
        "  for token in str:\n",
        "    lista.append(token)\n",
        "  \n",
        "\n",
        "  return lista\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuTsMmJMRy1Z",
        "colab_type": "code",
        "outputId": "df14aa3c-40d2-466d-f6b7-5352384d9d22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "sample = open(\"laptop_filtered_aspect_sample.csv\", \"r\") \n",
        "data = sample.read()\n",
        "data = data.replace(\" \",\"_\")\n",
        "data = data.replace(\"\\n\",\" \")\n",
        "data = data[12:700]\n",
        "\n",
        "data_list = str2list(str)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'laptop use computer screen price keyboard work battery product fast time battery_life notebook easy purchase works performance com machine looking chromebook very_good quality money light windows little speed size run problem apps games internet promotion school display issues review_was hours gaming set problems tablet os device design memory day ram daughter very_happy software power runs hard_drive programs update laptops features return touch_screen issue system college needs storage look great_laptop pc processor model keys sound weight ssd box lightweight usb start charge drive fan speakers great_product touchscreen very_fast bit upgrade son home cost long light_weight slow'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1E72S5s0P6vO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1 = gensim.models.Word2Vec(data) \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fnydjqb6Dsmi",
        "colab_type": "text"
      },
      "source": [
        "Testando o glove e o uso de um corpus\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKpR6Z7fVYuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "with open('laptop_filtered_aspect_sample.csv', 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    your_list = list(reader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUH9Rc-IZhY4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "flat_list = [item for sublist in your_list for item in sublist]\n",
        "np.shape(flat_list)\n",
        "size = len(flat_list)\n",
        "lista2 = [[] for _ in range(size)]\n",
        "\n",
        "for i  in range(1,size):\n",
        "  lista2[i-1].append(flat_list[i])\n",
        "\n",
        "lista2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cwb-QtIr5R7v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install glove_python"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy46n_UUCTYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "lista = [[] for _ in range(13)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwmCdgvXbNyt",
        "colab_type": "code",
        "outputId": "468488ca-3e14-4325-88cb-e24efb092f3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import nltk \n",
        "nltk.download() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> l\n",
            "\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "Hit Enter to continue: \n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "Hit Enter to continue: \n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [*] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "Hit Enter to continue: \n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "Hit Enter to continue: \n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [P] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [P] all................. All packages\n",
            "  [P] book................ Everything used in the NLTK Book\n",
            "  [P] popular............. Popular packages\n",
            "Hit Enter to continue: \n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages; [P] marks partially installed collections)\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B-dmQkApVkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import product_reviews_2\n",
        "print(len(product_reviews_2.words()))\n",
        "#stop_words = set(stopwords.words('english'))\n",
        "\n",
        "n = len(product_reviews_2.words())\n",
        "newcorpus = [[] for _ in range(n)]\n",
        "\n",
        "for i  in range(n):\n",
        "  newcorpus[i].append(product_reviews_2.words()[i])\n",
        "\n",
        "\n",
        "\n",
        "newcorpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjra7kM_JGIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " type"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_zaW2QKVgyt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing the glove library\n",
        "from glove import Corpus, Glove# creating a corpus object\n",
        "corpus = Corpus() #training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "\n",
        "corpus.fit(lista2, window=2, ignore_missing = True)#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "#We can set the learning rate as it uses Gradient Descent and number of componentsglove = Glove(no_components=5, learning_rate=0.05)\n",
        "glove = Glove(no_components=40, learning_rate=10)\n",
        "\n",
        "\n",
        "\n",
        "glove.fit(corpus.matrix, epochs= 500, no_threads=4, verbose=True)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.save('glove.model')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RIpuXdCH1GV",
        "colab_type": "code",
        "outputId": "60fd6cc8-3510-41ba-be5c-536088899ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.most_similar('display', 30)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('octane', 0.5592826631152799),\n",
              " ('keeper', 0.5307744661726197),\n",
              " ('low battery', 0.5210696201568729),\n",
              " ('no good', 0.515066418982208),\n",
              " ('plays games', 0.5107680310661425),\n",
              " ('designed', 0.4866701036093479),\n",
              " ('great response', 0.4792424162374582),\n",
              " ('simple stuff', 0.4728126669936721),\n",
              " ('laptop did', 0.4675606177601665),\n",
              " ('homes', 0.459383452587906),\n",
              " ('máquina', 0.45576666891968304),\n",
              " ('manufacturer', 0.45040654108379335),\n",
              " ('product was', 0.4476683166074003),\n",
              " ('weak', 0.4440465890004334),\n",
              " ('lb', 0.4435985418304988),\n",
              " ('old tablet', 0.4391451248091218),\n",
              " ('primary machine', 0.4372576474510182),\n",
              " ('separate mouse', 0.43638469665823554),\n",
              " ('working', 0.4341646789171227),\n",
              " ('in', 0.4299337168451753),\n",
              " ('extension', 0.428207918693019),\n",
              " ('styling', 0.4267154208821443),\n",
              " ('right computer', 0.42632809446713066),\n",
              " ('battery meter', 0.4258500183906589),\n",
              " ('external device', 0.4246891265099718),\n",
              " ('good time', 0.42209677077491586),\n",
              " ('transaction', 0.42049079926068234),\n",
              " ('so high', 0.41840505146781737),\n",
              " ('refurb', 0.41791589464161427)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "edJzBmCznSy-",
        "colab_type": "code",
        "outputId": "80724c16-0b34-4f7d-80bc-ca084cead85f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('different computer', 0.33111807798255416),\n",
              " ('notebook', 0.309549031434403),\n",
              " ('that great', 0.2871325709666734),\n",
              " ('small computer', 0.2714403758766526),\n",
              " ('usb port', 0.27064734284581005),\n",
              " ('premiere', 0.2653563458291859),\n",
              " ('finish', 0.26433128889510055),\n",
              " ('driver', 0.2598669671118168),\n",
              " ('hdd', 0.250470050187472),\n",
              " ('perfect product', 0.24951942126356264),\n",
              " ('card', 0.24537342251650693),\n",
              " ('same screen size', 0.24375465754945408),\n",
              " ('lcd', 0.24026497398289734),\n",
              " ('excellent buy', 0.23894363820416464),\n",
              " ('as smooth', 0.23832660618279594),\n",
              " ('magazines', 0.23684039632843734),\n",
              " ('wall socket', 0.2368350854369037),\n",
              " ('video playing', 0.2362373502433132)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib44H35XH5-U",
        "colab_type": "code",
        "outputId": "7912baea-694e-4bfe-c2cb-c549044aa833",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "print (glove.word_vectors[glove.dictionary['samsung']])\n",
        "j = 0\n",
        "#colocando a representação em vetor na lista\n",
        "for words in glove.dictionary:\n",
        "  lista[j].append(glove.word_vectors[glove.dictionary[i]])\n",
        "  j = j+1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-0.00882138  0.01401452  0.0129314   0.00016014  0.00104     0.01301976\n",
            " -0.01561628  0.00240939  0.00329525  0.01067138 -0.00532214  0.00077983\n",
            " -0.01322266  0.00378664  0.00969333 -0.0035686  -0.00824802 -0.00508309\n",
            "  0.01039907  0.01071623  0.00146902  0.00392297  0.00085903  0.01025379\n",
            "  0.00074051  0.00662573 -0.0061307  -0.01455731 -0.00614049  0.00483454]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufdqQxqoLl1z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "for i in range(len(lista)):\n",
        "  for j in range(len(lista)):\n",
        "     dst = (np.dot(lista[i][1], lista[j][1])/ np.linalg.norm(lista[i][1], axis=1) / np.linalg.norm(lista[j][1])\n",
        "\n",
        "\n",
        "    print(lista[i][0], lista[j][0],1 - spatial.distance.cosine(lista[i][1], lista[j][1]))  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZt4ZlQKCApF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(lista[:])\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dRtxppDMDVt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy import spatial"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYLN4LJJMD-D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#função recebe os primeiros pixels dos centroides e os retorna da forma correta que será utilizada no decorrer do código\n",
        "def initialCenter(dataset,centers):\n",
        "  newCenters = []\n",
        "  for i in range(len(centers)):\n",
        "    newCenters.append(([]))\n",
        "    newCenters[i].append(dataset[centers[i]])\n",
        "  return newCenters\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1ibEv7SRa9n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#função que recebe dois numpy array e retorna o índice da menor diferença encontrada entre o pixel e os centroides\n",
        "def distance(centers,word_array, j):\n",
        "  result = [[] for _ in range(len(centers))]\n",
        "  aux = []\n",
        "  similarity = [len(centers)]\n",
        "  for i in range(len(centers)):\n",
        "    similarity[i] = 1-spatial.distance.cosine(word_array[centers[i]][1], word_array[j][1])    \n",
        "  #retorna o índice da menor distância\n",
        "\n",
        "  return(centers[np.argmax(similarity)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHE9BN5wMD7A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def KMEANS (k,word_vectors,n):\n",
        "  #n é a quantidade de iteração\n",
        "  size = len(word_vectors) \n",
        "\n",
        "\n",
        "  ## possui o número do pixels dos centros\n",
        "  centroids = np.sort(random.sample(range(0, size), k))\n",
        "    \n",
        "  ## possui os clusters de cada pixels\n",
        "  labels = np.zeros(size)\n",
        "  clusters = [] \n",
        "  \n",
        "  for i in range (k): \n",
        "    clusters.append(([]))\n",
        "\n",
        "\n",
        "\n",
        "  for i in range(n):\n",
        "    for j in range(size):\n",
        "      \n",
        "      \n",
        "      #conversão para nparray para utilização de funções np a seguir\n",
        "      npData = np.array(word_vectors[j][1])\n",
        "      npCenter = np.array(centroids)\n",
        "      ##labels[j] vai receber o índice da menor diferença entre o pixel atual e os centroidos\n",
        "      labels[j].append(distance(npCenter,npData))\n",
        "      \n",
        "      #armazena no cluster todos os pixels da mesma classe\n",
        "      clusters[int(labels[j])].append(word_vectors[j])\n",
        "\n",
        "\n",
        "    for z in range(k):\n",
        "      centers[z] = np.mean(clusters[z], axis=0)\n",
        "  ##transformando a lista em uma matriz novamente\n",
        "  labels = labels.reshape(x,y)\n",
        "\n",
        "  return labels\n",
        "    \n",
        "       \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_XzmgjoMD5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clusterMean(j,cluster, k, dimension)\n",
        "  array = [k]\n",
        "  for i in range (k):\n",
        "    array[i]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lewBHjU6MD2k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Clustering (word_vectors,alpha):\n",
        "  size = len(word_vectors)\n",
        "  cluster = []\n",
        "\n",
        "  for i in range(size):\n",
        "    for j in range(i,size):\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YERiFLoAMDzq",
        "colab_type": "code",
        "outputId": "ccd2fc66-7542-47c5-a202-09051f1fea6a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word_vectors)\n",
        "a = [[]]\n",
        "\n",
        "a"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aculTPdQMDwx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKowJBxuMDuV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT4tySlFMDrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JpzcfKuMDpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejkVZ-GRMDmU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQd_DRkVMDkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btql79y8MDh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0pRccAjMDfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import product_reviews_2\n",
        "product_reviews_2.words()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z67GdJWGEhvs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords stop_words=set(stopwords.words(‘english’)) \n",
        "lines_without_stopwords=[] #stop words contain the set of stop words \n",
        "for line in lines: temp_line=[] \n",
        "for word in lines: \n",
        "  if word not in stop_words: \n",
        "    temp_line.append (word) \n",
        "    string=’ ‘ \n",
        "lines_without_stopwords.append(string.join(temp_line))\n",
        " lines=lines_without_stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIIIUNsNIn6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_lines = [] \n",
        "for line in lines: \n",
        "  new_lines = line.split(‘’) #new lines has the new format lines=new_lines"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}